---
name: mlops-pipeline
description: Boas prÃ¡ticas de MLOps â€” treinamento, versionamento de modelos, deploy de ML, monitoramento de drift, pipelines de dados e feature stores.
triggers:
  - mlops
  - machine learning
  - modelo
  - treinamento
  - training
  - deploy de modelo
  - model serving
  - feature store
  - data pipeline
  - drift
  - experiment tracking
  - mlflow
  - wandb
  - kubeflow
  - bentoml
  - rag
  - fine-tuning
  - embeddings
  - vetor
  - vector database
---

# MLOps Pipeline

## Objetivo
Implementar e gerenciar pipelines de Machine Learning em produÃ§Ã£o, cobrindo todo o ciclo: dados â†’ treinamento â†’ avaliaÃ§Ã£o â†’ deploy â†’ monitoramento â†’ retraining.

## Contexto necessÃ¡rio
- Tipo de modelo (classificaÃ§Ã£o, NLP, visÃ£o, LLM, recomendaÃ§Ã£o)
- Framework (PyTorch, TensorFlow, scikit-learn, HuggingFace)
- Infraestrutura (local, cloud, GPU)
- EstÃ¡gio atual (exploraÃ§Ã£o, staging, produÃ§Ã£o)

## Fluxo (inspect â†’ plan â†’ consent â†’ apply â†’ verify â†’ audit)

1. **INSPECT**: Analisar pipeline existente, dados, modelos e infra
2. **PLAN**: Propor arquitetura MLOps com componentes necessÃ¡rios
3. **CONSENT**: Confirmar custos de compute e storage
4. **APPLY**: Implementar/modificar pipeline
5. **VERIFY**: Validar mÃ©tricas, latÃªncia, throughput
6. **AUDIT**: Registrar experimentos, versÃµes e decisÃµes

## Capacidades

### ğŸ“Š Experiment Tracking
- MLflow: experiments, runs, parÃ¢metros, mÃ©tricas, artefatos
- Weights & Biases (W&B): tracking, sweeps, reports
- ComparaÃ§Ã£o entre runs e reprodutibilidade

### ğŸ“¦ Versionamento de Modelos e Dados
- DVC: versionamento de datasets grandes
- MLflow Model Registry: staging â†’ production
- Git LFS para artefatos pesados
- Hashes de datasets para reprodutibilidade

### ğŸ”„ Pipelines de Treinamento
- OrquestraÃ§Ã£o: Airflow, Prefect, Kubeflow Pipelines
- Feature engineering automatizado
- ValidaÃ§Ã£o de dados (Great Expectations, Pandera)
- Hyperparameter tuning (Optuna, Ray Tune)

### ğŸš€ Model Serving
- APIs REST/gRPC: FastAPI + ONNX, TorchServe, TF Serving
- BentoML: empacotamento e deploy de modelos
- Serverless: AWS Lambda + SageMaker, GCP Cloud Functions
- Edge: ONNX Runtime, TensorFlow Lite

### ğŸ” Monitoramento em ProduÃ§Ã£o
- Data drift detection (Evidently, NannyML)
- Model performance monitoring (accuracy decay)
- LatÃªncia e throughput (P50, P95, P99)
- Alertas para retraining automÃ¡tico

### ğŸ§  LLM Ops (RAG, Fine-tuning, Agents)
- RAG pipelines: embeddings â†’ vector DB â†’ retrieval â†’ generation
- Vector databases: Qdrant, ChromaDB, Pinecone, Weaviate
- Fine-tuning: LoRA, QLoRA, em GPUs de consumo
- AvaliaÃ§Ã£o de LLMs: BLEU, ROUGE, human eval, LLM-as-judge
- Guardrails: content filtering, prompt injection detection

## Checklists

### Antes de treinar
- [ ] Dados validados (schema, distribuiÃ§Ã£o, missing values)
- [ ] Split reprodutÃ­vel (train/val/test com seed fixa)
- [ ] Baseline definido (modelo simples para comparaÃ§Ã£o)
- [ ] MÃ©tricas de avaliaÃ§Ã£o escolhidas e documentadas
- [ ] Experiment tracking configurado

### Antes de deploy
- [ ] Modelo versionado com metadados (hash, mÃ©tricas, dataset)
- [ ] Testes de integraÃ§Ã£o (input â†’ output esperado)
- [ ] Benchmark de latÃªncia e throughput
- [ ] Fallback definido (modelo anterior ou regra heurÃ­stica)
- [ ] Monitoramento de drift configurado

### Em produÃ§Ã£o
- [ ] Alertas para degradaÃ§Ã£o de performance
- [ ] Pipeline de retraining automatizado ou semi-automÃ¡tico
- [ ] A/B testing ou shadow mode para novos modelos
- [ ] Logs de prediÃ§Ãµes para auditoria e debugging
- [ ] Custo de compute monitorado

## Regras de seguranÃ§a
- âœ… Dados sensÃ­veis devem ser anonimizados/mascarados antes de treinar
- âœ… Modelos devem ser escaneados para bias antes de deploy
- âœ… API keys de provedores de LLM devem usar secret management
- âŒ Nunca expor endpoints de model serving sem autenticaÃ§Ã£o
- âŒ Nunca treinar com dados de produÃ§Ã£o sem aprovaÃ§Ã£o de compliance
